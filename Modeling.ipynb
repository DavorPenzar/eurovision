{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a05528d-1c77-482b-b8c2-b83b55f53334",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A Good Modeling Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233c8a3-08fe-4fb4-b067-6aaf4a6bb3c4",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Environment Initialisation](#Environment-Initialisation)\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "    1. [Synthetic Minority Oversampling Technique (SMOTE)](#Synthetic-Minority-Oversampling-Technique-%28SMOTE%29)\n",
    "    2. [Correlation Reduction](#Correlation-Reduction)\n",
    "3. [Model Construction](#Model-Construction)\n",
    "    1. [Hyperparameter Optimisation](#Hyperparameter-Construction)\n",
    "    2. [Final Model](#Final-Model)\n",
    "    3. [Testing](#Testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e1b7d-b01f-491c-89f1-c02c65e82bdd",
   "metadata": {},
   "source": [
    "## Environment Initialisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32822e12-a337-4014-ab51-08572070ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2bc8e8-3b0b-4759-a413-95c055def67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numbers\n",
    "import os\n",
    "import pickle\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14477e59-7a16-4044-9963-940af57cd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.optimize as opt\n",
    "import scipy.special as spec\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27bd5b-efd1-4595-b963-c0d87c01085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "import sklearn.decomposition as skldec\n",
    "import sklearn.linear_model as skllm\n",
    "import sklearn.metrics as sklmet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a552839-8b37-4ec4-b891-46dbb368bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.metrics as metrics\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "import tensorflow.keras.utils as utils\n",
    "import tensorflow_addons.losses as tfa_losses\n",
    "import tensorflow_addons.metrics as tfa_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71511553-1c80-47fb-a2fc-af7f534675cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt as hopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50c7bf-6d8a-45c1-8d2b-b5cb317b3851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4dcc37-f890-4742-89f4-e9b382fbc891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d5ca8-7903-4cc8-90d6-510a16fa794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "sns.set_theme()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5fb342-5bd0-49d6-b323-684eaf126fe9",
   "metadata": {},
   "source": [
    "The following versions were used in the development of this *Jupyter Notebook*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48264f91-56a2-4cc3-92ac-d35c182bf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Python version: \" \\\n",
    "        f\"{sys.version_info.major}.\" \\\n",
    "        f\"{sys.version_info.minor}.\" \\\n",
    "        f\"{sys.version_info.micro}\"\n",
    ")\n",
    "\n",
    "print(f\"Scipy version: {sp.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {mpl.__version__}\")\n",
    "\n",
    "print(f\"scikit-learn version: {skl.__version__}\")\n",
    "\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "print(f\"Hyperopt version: {hopt.__version__}\")\n",
    "\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ecc4e-649a-486b-8614-b3e145afc5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as myutils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a1503-49bd-4068-8d12-bd207040e2e3",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd3d1c8-8d3f-458d-9f31-c062df40118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'data'\n",
    "arrays_root = 'arrays'\n",
    "models_root = 'models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737c759-5f22-4b2e-a3ea-a5fcc7c70d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.path.join(data_root, 'scores.csv'),\n",
    "    sep = ';',\n",
    "    header = 0,\n",
    "    index_col = None,\n",
    "    decimal = ','\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e37b3d-c667-402f-b846-74335cc20389",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\n",
    "    os.path.join(data_root, 'labels.csv'),\n",
    "    sep = ';',\n",
    "    header = 0,\n",
    "    index_col = None,\n",
    "    decimal = ','\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8d90c-e770-4740-b97c-9b42ae07d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(labels, how = 'left', on = 'country')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f1cfc-0418-40c8-9928-175fbd0f1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "del labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96074224-2244-4ce8-9fd6-2743471e6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(\n",
    "    [\n",
    "        'year',\n",
    "        'country',\n",
    "        'label',\n",
    "        'score',\n",
    "        'nscore',\n",
    "        'rank'\n",
    "    ],\n",
    "    axis = 1\n",
    ").reset_index(drop = True).copy(deep = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f7b2b-31af-4805-b3b0-219d71c50302",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.random.default_rng(2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb421d-7f3f-444e-b952-d3eb4c07dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1990\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568a11c-45a4-4d74-8ad8-a99357c2a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list()\n",
    "year_train = list()\n",
    "time_train = list()\n",
    "pos_train = list()\n",
    "diverse_train = list()\n",
    "y_train = list()\n",
    "\n",
    "X_valid = list()\n",
    "year_valid = list()\n",
    "time_valid = list()\n",
    "pos_valid = list()\n",
    "diverse_valid = list()\n",
    "y_valid = list()\n",
    "\n",
    "for y in range(year - 10, year):\n",
    "    trains = np.load(os.path.join(arrays_root, str(y), 'train.npy'))\n",
    "\n",
    "    I = df.index[df.year == y]\n",
    "\n",
    "    for i in I:\n",
    "        c = str(np.asarray(df.loc[i, 'country']).item())\n",
    "        a = np.asarray(df.loc[i, 'year']).item()\n",
    "        s = np.asarray(df.loc[i, 'nscore']).item()\n",
    "\n",
    "        x = np.load(os.path.join(arrays_root, str(y), c, 'taggram.npy'))\n",
    "        t = np.load(os.path.join(arrays_root, str(y), c, 't.npy'))\n",
    "        p = np.load(os.path.join(arrays_root, str(y), c, 'p.npy'))\n",
    "        d = np.load(os.path.join(arrays_root, str(y), c, 'diverse.npy'))\n",
    "\n",
    "        if np.asarray(df.loc[i, 'label']).item() in trains:\n",
    "            X_train.append(x)\n",
    "            year_train.append(np.full(x.shape[0], a, dtype = x.dtype))\n",
    "            time_train.append(np.full(x.shape[0], t, dtype = x.dtype))\n",
    "            pos_train.append(np.full(x.shape[0], p, dtype = x.dtype))\n",
    "            diverse_train.append(np.full(x.shape[0], d, dtype = x.dtype))\n",
    "            y_train.append(np.full(x.shape[0], s, dtype = x.dtype))\n",
    "        else:\n",
    "            X_valid.append(x)\n",
    "            year_valid.append(np.full(x.shape[0], a, dtype = x.dtype))\n",
    "            time_valid.append(np.full(x.shape[0], t, dtype = x.dtype))\n",
    "            pos_valid.append(np.full(x.shape[0], p, dtype = x.dtype))\n",
    "            diverse_valid.append(np.full(x.shape[0], d, dtype = x.dtype))\n",
    "            y_valid.append(np.full(x.shape[0], s, dtype = x.dtype))\n",
    "\n",
    "X_train = np.concatenate(X_train, axis = 0)\n",
    "year_train = np.squeeze(np.concatenate(year_train, axis = 0))\n",
    "time_train = np.squeeze(np.concatenate(time_train, axis = 0))\n",
    "pos_train = np.squeeze(np.concatenate(pos_train, axis = 0))\n",
    "diverse_train = np.squeeze(np.concatenate(diverse_train, axis = 0))\n",
    "y_train = np.squeeze(np.concatenate(y_train, axis = 0))\n",
    "\n",
    "X_valid = np.concatenate(X_valid, axis = 0)\n",
    "year_valid = np.squeeze(np.concatenate(year_valid, axis = 0))\n",
    "time_valid = np.squeeze(np.concatenate(time_valid, axis = 0))\n",
    "pos_valid = np.squeeze(np.concatenate(pos_valid, axis = 0))\n",
    "diverse_valid = np.squeeze(np.concatenate(diverse_valid, axis = 0))\n",
    "y_valid = np.squeeze(np.concatenate(y_valid, axis = 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250eb5c9-f248-4aee-b689-3044b8a13bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = R.permutation(y_train.shape[0])\n",
    "\n",
    "X_train = X_train[I].copy()\n",
    "year_train = year_train[I].copy()\n",
    "time_train = time_train[I].copy()\n",
    "pos_train = pos_train[I].copy()\n",
    "diverse_train = diverse_train[I].copy()\n",
    "y_train = y_train[I].copy()\n",
    "\n",
    "I = R.permutation(y_valid.shape[0])\n",
    "\n",
    "X_valid = X_valid[I].copy()\n",
    "year_valid = year_valid[I].copy()\n",
    "time_valid = time_valid[I].copy()\n",
    "pos_valid = pos_valid[I].copy()\n",
    "diverse_valid = diverse_valid[I].copy()\n",
    "y_valid = y_valid[I].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7380ed4-d1e4-4e3a-8558-dc7d16f9315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df_train = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            X_train,\n",
    "            np.expand_dims(year_train.astype(np.float32), axis = 1),\n",
    "            np.expand_dims(time_train, axis = 1),\n",
    "            np.expand_dims(pos_train, axis = 1),\n",
    "            np.expand_dims(diverse_train, axis = 1),\n",
    "            np.expand_dims(y_train, axis = 1)\n",
    "        ),\n",
    "        axis = 1\n",
    "    ),\n",
    "    columns = pd.Index(\n",
    "        list(f\"p_{i + 1:d}\" for i in range(int(X_train.shape[1]))) +\n",
    "            [ 'y', 't', 'p', 'd', 's' ],\n",
    "        dtype = np.str_,\n",
    "        name = 'feature'\n",
    "    )\n",
    ")\n",
    "\n",
    "feat_df_valid = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            X_valid,\n",
    "            np.expand_dims(year_valid, axis = 1),\n",
    "            np.expand_dims(time_valid, axis = 1),\n",
    "            np.expand_dims(pos_valid, axis = 1),\n",
    "            np.expand_dims(diverse_valid, axis = 1),\n",
    "            np.expand_dims(y_valid, axis = 1)\n",
    "        ),\n",
    "        axis = 1\n",
    "    ),\n",
    "    columns = pd.Index(\n",
    "        list(f\"p_{i + 1:d}\" for i in range(int(X_train.shape[1]))) +\n",
    "            [ 'y', 't', 'p', 'd', 's' ],\n",
    "        dtype = np.str_,\n",
    "        name = 'feature'\n",
    "    )\n",
    ")\n",
    "\n",
    "mcnnt_col = pd.Index(\n",
    "    list(f\"p_{i + 1:d}\" for i in range(int(X_train.shape[1]))),\n",
    "    dtype = np.str_,\n",
    "    name = 'feature'\n",
    ")\n",
    "pos_col = 'p'\n",
    "time_col = 't'\n",
    "diverse_col = 'd'\n",
    "year_col = 'y'\n",
    "score_col = 's'\n",
    "\n",
    "X_col = pd.Index(\n",
    "    list(mcnnt_col) + [ 'p', 't', 'd', 'y' ],\n",
    "    dtype = np.str_,\n",
    "    name = 'feature'\n",
    ")\n",
    "X_scalar_col = pd.Index(\n",
    "    [ 'p', 't', 'd', 'y' ],\n",
    "    dtype = np.str_,\n",
    "    name = 'feature'\n",
    ")\n",
    "y_col = score_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9d470-d3a3-4c2f-bc1c-520c3c0b1a8b",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling Technique (SMOTE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9eae4-72cb-4288-bf6c-eafe1a0442dc",
   "metadata": {},
   "source": [
    "SMOTE is done by oversampling minority bins in histogram. Histogram is first done in $ 2 $ equally wide bins (halves), then $ 3 $ (thirds) and so on; in each iteration, all bins that have less than the maximal number of samples are oversampled using SMOTE. SMOTE skips sampling from a combination of samples $ x $ and $ y $ if their years are more than $ 2 $ apart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa30d7-09d9-4660-84ef-71f4e2f5bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE (feat_df, low = 0, high = 5.5, max_bins = 12):\n",
    "    for b in range(2, max_bins + 1):\n",
    "        bin_edges = np.linspace(low, high, num = b + 1, dtype = np.float32)\n",
    "        bin_edges[-1] += 1.0e-8\n",
    "\n",
    "        hist, _ = np.histogram(\n",
    "            feat_df['s'],\n",
    "            bins = bin_edges,\n",
    "            density = False\n",
    "        )\n",
    "\n",
    "        n_max = np.amax(hist)\n",
    "\n",
    "        for k in range(b):\n",
    "            if hist[k] < 2 or hist[k] == n_max:\n",
    "                continue\n",
    "\n",
    "            ind = feat_df.index[\n",
    "                (feat_df['s'] >= bin_edges[k]) &\n",
    "                (feat_df['s'] < bin_edges[k + 1])\n",
    "            ]\n",
    "            years = np.asarray(feat_df.loc[ind, 'y'].values)\n",
    "            year_diffs = np.absolute(\n",
    "                np.expand_dims(years, axis = 0) -\n",
    "                    np.expand_dims(years, axis = 1)\n",
    "            )\n",
    "            year_diffs = (year_diffs > 2)\n",
    "\n",
    "            n_diff = n_max - hist[k]\n",
    "\n",
    "            I = np.zeros((n_diff, 2), dtype = np.int32)\n",
    "            while True:\n",
    "                J = ((I[:, 0] == I[:, 1]) | year_diffs[I[:, 0], I[:, 1]])\n",
    "                n_g = np.sum(J)\n",
    "\n",
    "                if not n_g:\n",
    "                    break\n",
    "\n",
    "                I[J] = R.choice(hist[k], size = (n_g, 2), replace = True)\n",
    "\n",
    "            t = R.uniform(0, 1, size = (n_diff, 1))\n",
    "\n",
    "            gen_df = \\\n",
    "                t * feat_df.loc[ind[I[:, 0]]].reset_index(drop = True) + \\\n",
    "                    (1 - t) * feat_df.loc[ind[I[:, 1]]].reset_index(drop = True)\n",
    "\n",
    "            feat_df = pd.concat(\n",
    "                (feat_df, gen_df),\n",
    "                axis = 0,\n",
    "                ignore_index = True\n",
    "            )\n",
    "\n",
    "    return feat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f80817-c1d3-46e8-84ff-34530601ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(feat_df_train.shape)\n",
    "display(feat_df_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0e451-2d2f-474c-809f-e0d6f1b9b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df_train = SMOTE(feat_df_train, max_bins = 12)\n",
    "feat_df_valid = SMOTE(feat_df_valid, max_bins = 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86807523-6e54-4c40-a2df-88f7c71fe308",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(feat_df_train.shape)\n",
    "display(feat_df_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160473d5-c11f-489f-811e-bbb111e6f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df_train['y'] = np.around(feat_df_train['y']).astype(np.int32)\n",
    "feat_df_valid['y'] = np.around(feat_df_valid['y']).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4bcd3-121b-40a3-bdea-eea02968bdce",
   "metadata": {},
   "source": [
    "### Correlation Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75aa8e4-a758-4d48-b30f-0da3ee2a9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_correlations (X, **fig_kwargs):\n",
    "    fig = plt.figure(**fig_kwargs)\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    corr, corr_p = stats.spearmanr(X, axis = 0)\n",
    "    corr[np.eye(corr.shape[0], dtype = np.bool_)] = 0\n",
    "\n",
    "    corr_mask = np.triu(np.ones_like(corr, dtype = np.bool_))\n",
    "\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        mask = corr_mask,\n",
    "        vmin = -1,\n",
    "        vmax = 1,\n",
    "        center = 0,\n",
    "        square = True,\n",
    "        linewidths = 0,\n",
    "        cmap = 'coolwarm'\n",
    "    )\n",
    "\n",
    "    corr_aux = corr.copy(order = 'C')\n",
    "\n",
    "    corr_aux[~corr_mask] = -np.inf\n",
    "    i_max = np.unravel_index(np.argmax(corr_aux), shape = corr.shape, order = 'C')\n",
    "\n",
    "    corr_aux[~corr_mask] = np.inf\n",
    "    i_min = np.unravel_index(np.argmin(corr_aux), shape = corr.shape, order = 'C')\n",
    "\n",
    "    print(f\"{i_max[0]}, {i_max[1]}: {corr[i_max]:.2%}\")\n",
    "    print(f\"{i_min[0]}, {i_min[1]}: {corr[i_min]:.2%}\")\n",
    "\n",
    "    return (corr, fig, ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f80955-e842-42ab-860c-a194e55f60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_correlations(\n",
    "    feat_df_train.loc[:, mcnnt_col],\n",
    "    figsize = (16, 16)\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a68943-0c50-4804-a957-cb327dd75732",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = skldec.TruncatedSVD(\n",
    "    n_components = X_train.shape[1] - 1,\n",
    "    random_state = R.integers(np.iinfo(np.int32).max)\n",
    ")\n",
    "svd.fit(feat_df_train.loc[:, mcnnt_col])\n",
    "\n",
    "n = np.argmax(np.cumsum(svd.explained_variance_ratio_) >= 0.95)\n",
    "\n",
    "M = svd.components_[:n].copy(order = 'C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5b67f-811e-44d4-8e7b-dea08696fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bb55e-ad92-450f-897e-73dee0ab1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_correlations(\n",
    "    np.matmul(feat_df_train.loc[:, mcnnt_col], M.T),\n",
    "    figsize = (16, 16)\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196bea8-958f-463c-82a2-af1a5068b391",
   "metadata": {},
   "source": [
    "## Model Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84125e-7745-4051-a455-f3f88d153232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score (y_true, y_pred, sample_weight = None):\n",
    "    if sample_weight is None:\n",
    "        sample_weight = K.ones_like(y_true, dtype = 'float32')\n",
    "\n",
    "    sample_weight = sample_weight / K.sum(sample_weight)\n",
    "\n",
    "    y_mean = K.sum(sample_weight * y_true)\n",
    "\n",
    "    SS_res = K.sum(sample_weight * K.square(y_true - y_pred))\n",
    "    SS_tot = K.maximum(\n",
    "        K.sum(sample_weight * K.square(y_true - y_mean)),\n",
    "        K.epsilon()\n",
    "    )\n",
    "\n",
    "    return 1 - SS_res / SS_tot\n",
    "\n",
    "def neg_r2_score (y_true, y_pred, sample_weight = None):\n",
    "    return -r2_score(y_true, y_pred, sample_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c2805-f7d7-44e9-872c-1dc80673c054",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc044fd4-b2a2-4bb7-994a-a3685530a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model (args):\n",
    "    \"\"\"Returns inputs, outputs and number of parameters.\"\"\"\n",
    "    m_in = 0\n",
    "    m_out = 0\n",
    "    n = 0\n",
    "\n",
    "    inputs = layers.Input(\n",
    "        shape = M.shape[0] + X_scalar_col.size,\n",
    "        name = 'input'\n",
    "    )\n",
    "    outputs = inputs\n",
    "\n",
    "    m_in = M.shape[0] + X_scalar_col.size\n",
    "\n",
    "    j = 1\n",
    "    for i in range(6):\n",
    "        m_out = int(round(args[f\"hidden_{j:d}\"]))\n",
    "\n",
    "        if m_out > 1:\n",
    "            outputs = layers.Dense(\n",
    "                units = m_out,\n",
    "                activation = 'relu',\n",
    "                name = f\"hidden_{j}\"\n",
    "            )(outputs)\n",
    "\n",
    "            n += m_out * (m_in + 1)\n",
    "\n",
    "            m_in = m_out\n",
    "\n",
    "            j += 1\n",
    "\n",
    "    outputs = layers.Dense(\n",
    "        units = 1,\n",
    "        activation = 'linear',\n",
    "        name = 'output'\n",
    "    )(outputs)\n",
    "\n",
    "    n += m_in + 1\n",
    "\n",
    "    return (inputs, outputs, n)\n",
    "\n",
    "def objective (args):\n",
    "    inputs, outputs, n = construct_model(args)\n",
    "\n",
    "    if n > 2400:\n",
    "        return { 'status': hopt.STATUS_FAIL }\n",
    "\n",
    "    model = models.Model(\n",
    "        inputs = inputs,\n",
    "        outputs = outputs,\n",
    "        name = 'model'\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = optimizers.Adadelta(\n",
    "            learning_rate = 1.0e-2,\n",
    "            name = 'optimiser'\n",
    "        ),\n",
    "        loss = 'mean_squared_error',\n",
    "        metrics = [ r2_score ]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x = np.concatenate(\n",
    "            (\n",
    "                np.matmul(feat_df_train[mcnnt_col], M.T),\n",
    "                feat_df_train[X_scalar_col]\n",
    "            ),\n",
    "            axis = 1\n",
    "        ),\n",
    "        y = feat_df_train[y_col],\n",
    "        validation_data = (\n",
    "            np.concatenate(\n",
    "                (\n",
    "                    np.matmul(feat_df_valid[mcnnt_col], M.T),\n",
    "                    feat_df_valid[X_scalar_col]\n",
    "                ),\n",
    "                axis = 1\n",
    "            ),\n",
    "            feat_df_valid[y_col]\n",
    "        ),\n",
    "        shuffle = True,\n",
    "        batch_size = 256,\n",
    "        epochs = 100,\n",
    "        verbose = 0,\n",
    "        use_multiprocessing = False,\n",
    "        callbacks = [\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.EarlyStopping(\n",
    "                monitor = 'val_loss',\n",
    "                patience = 4,\n",
    "                mode = 'min',\n",
    "                restore_best_weights = True\n",
    "            ),\n",
    "            callbacks.ReduceLROnPlateau(\n",
    "                monitor = 'val_loss',\n",
    "                patience = 2,\n",
    "                mode = 'min',\n",
    "                factor = 0.5\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    loss = np.asanyarray(history.history['val_loss'])\n",
    "    r2 = np.asanyarray(history.history['val_r2_score'])\n",
    "\n",
    "    return {\n",
    "        'status': hopt.STATUS_OK,\n",
    "        'nparams': n,\n",
    "        'loss': -math.log(n) * r2[np.argmin(loss)]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d80e76-73a5-4e8c-9a25-cb3d30c24f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the number of units to 1 will actually omit the hidden layer.\n",
    "space = dict(\n",
    "    (\n",
    "        f\"hidden_{i + 1:d}\",\n",
    "        hopt.hp.quniform(\n",
    "            f\"hidden_{i + 1:d}\",\n",
    "            1,\n",
    "            int(round(math.ceil(25.0 / (i + 1)))),\n",
    "            1\n",
    "        )\n",
    "    ) for i in range(6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596f247-055a-45d4-ae9b-df39eba18353",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = hopt.Trials()\n",
    "best = hopt.fmin(\n",
    "    objective,\n",
    "    space = space,\n",
    "    algo = hopt.tpe.suggest,\n",
    "    max_evals = 500,\n",
    "    trials = trials\n",
    ")\n",
    "with open('hyperopt_trials.pkl', 'wb') as trials_output:\n",
    "    pickle.dump(trials, trials_output, fix_imports = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb55094-94ad-4fc3-bb28-02fbb65d9261",
   "metadata": {},
   "outputs": [],
   "source": [
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c202557-8ed6-4920-8555-b39b1ed07596",
   "metadata": {},
   "source": [
    "### Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181eac34-4b0c-4c26-a844-358ee118719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs, n = construct_model(best)\n",
    "\n",
    "model = models.Model(\n",
    "    inputs = inputs,\n",
    "    outputs = outputs,\n",
    "    name = 'model'\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizers.Adadelta(\n",
    "        learning_rate = 1.0e-3,\n",
    "        name = 'optimiser'\n",
    "    ),\n",
    "    loss = 'mean_squared_error',\n",
    "    metrics = [\n",
    "        metrics.RootMeanSquaredError(),\n",
    "        r2_score,\n",
    "        metrics.LogCoshError(),\n",
    "        metrics.CosineSimilarity()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea6d72-13f4-4878-9d95-00312fa7475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4017b41-23ed-40e3-95c8-96a77878cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88dbdcc-abb8-430d-a0b8-8e4598a69395",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x = np.concatenate(\n",
    "        (\n",
    "            np.matmul(feat_df_train[mcnnt_col], M.T),\n",
    "            feat_df_train[X_scalar_col]\n",
    "        ),\n",
    "        axis = 1\n",
    "    ),\n",
    "    y = feat_df_train[y_col],\n",
    "    validation_data = (\n",
    "        np.concatenate(\n",
    "            (\n",
    "                np.matmul(feat_df_valid[mcnnt_col], M.T),\n",
    "                feat_df_valid[X_scalar_col]\n",
    "            ),\n",
    "            axis = 1\n",
    "        ),\n",
    "        feat_df_valid[y_col]\n",
    "    ),\n",
    "    shuffle = True,\n",
    "    batch_size = 256,\n",
    "    epochs = 2048,\n",
    "    verbose = 0,\n",
    "    use_multiprocessing = False,\n",
    "    callbacks = [\n",
    "        callbacks.TerminateOnNaN(),\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss',\n",
    "            patience = 64,\n",
    "            mode = 'min',\n",
    "            restore_best_weights = True\n",
    "        ),\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor = 'val_loss',\n",
    "            patience = 2,\n",
    "            mode = 'min',\n",
    "            factor = 0.5\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5eec4-d7e3-4af7-8496-ac9d2cba17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(history.history['loss'])\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, sharey = False, figsize = (16, 9))\n",
    "\n",
    "ax[0].plot(\n",
    "    list(range(1, k + 1)),\n",
    "    history.history['root_mean_squared_error'][:k]\n",
    ")\n",
    "ax[0].plot(\n",
    "    list(range(1, k + 1)),\n",
    "    history.history['val_root_mean_squared_error'][:k]\n",
    ")\n",
    "\n",
    "ax[1].plot(\n",
    "    list(range(1, k + 1)),\n",
    "    history.history['r2_score'][:k]\n",
    ")\n",
    "ax[1].plot(\n",
    "    list(range(1, k + 1)),\n",
    "    history.history['val_r2_score'][:k]\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e1dfe-964c-4893-bf75-437f30886f4f",
   "metadata": {},
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff7985-0344-4899-82bd-b99b63c65b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.loc[df.year == year].copy(deep = True)\n",
    "df_test['nscore_pred'] = np.zeros(df_test.shape[0])\n",
    "\n",
    "pred_test = list()\n",
    "\n",
    "for i in df.index[df.year == year]:\n",
    "    X_test = list()\n",
    "    year_test = list()\n",
    "    time_test = list()\n",
    "    pos_test = list()\n",
    "    diverse_test = list()\n",
    "    y_test = None\n",
    "\n",
    "    c = str(np.asarray(df.loc[i, 'country']).item())\n",
    "    a = np.asarray(df.loc[i, 'year']).item()\n",
    "    s = np.asarray(df.loc[i, 'nscore']).item()\n",
    "\n",
    "    x = np.load(os.path.join(arrays_root, str(year), c, 'taggram.npy'))\n",
    "    t = np.load(os.path.join(arrays_root, str(year), c, 't.npy'))\n",
    "    p = np.load(os.path.join(arrays_root, str(year), c, 'p.npy'))\n",
    "    d = np.load(os.path.join(arrays_root, str(year), c, 'diverse.npy'))\n",
    "\n",
    "    X_test = x\n",
    "    year_test = np.full((x.shape[0], 1), a, dtype = x.dtype)\n",
    "    time_test = np.full((x.shape[0], 1), t, dtype = x.dtype)\n",
    "    pos_test = np.expand_dims(p, axis = 1)\n",
    "    diverse_test = np.full((x.shape[0], 1), d, dtype = x.dtype)\n",
    "    y_test = s\n",
    "\n",
    "    y_test_pred = model.predict(\n",
    "        np.concatenate(\n",
    "            (\n",
    "                np.matmul(X_test, M.T),\n",
    "                pos_test - 0.5,\n",
    "                time_test,\n",
    "                diverse_test,\n",
    "                year_test - (year - 5.5)\n",
    "            ),\n",
    "            axis = 1\n",
    "        ),\n",
    "        batch_size = 64\n",
    "    ).ravel()\n",
    "\n",
    "    df_test.loc[i, 'nscore_pred'] = np.quantile(y_test_pred, q = 0.25)\n",
    "\n",
    "    pred_test.append((y_test, y_test_pred))\n",
    "\n",
    "df_test.sort_values(\n",
    "    by = [ 'score', 'country' ],\n",
    "    ascending = [ False, True ],\n",
    "    inplace = True\n",
    ")\n",
    "df_test.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a4875-d33c-4520-b9b2-448d061c4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16, 9))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(\n",
    "    list(p[0] for p in pred_test),\n",
    "    list(np.quantile(p[1], q = 0.75) for p in pred_test),\n",
    "    color = 'C0',\n",
    "    alpha = 0.1,\n",
    "    s = 16\n",
    ")\n",
    "ax.scatter(\n",
    "    list(p[0] for p in pred_test),\n",
    "    list(np.quantile(p[1], q = 0.50) for p in pred_test),\n",
    "    color = 'C2',\n",
    "    alpha = 0.1,\n",
    "    s = 16\n",
    ")\n",
    "ax.scatter(\n",
    "    list(p[0] for p in pred_test),\n",
    "    list(np.quantile(p[1], q = 0.25) for p in pred_test),\n",
    "    color = 'C1',\n",
    "    alpha = 1,\n",
    "    s = 32\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd65cf-b036-43fa-97ee-7940fda40716",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklmet.r2_score(\n",
    "    list(p[0] for p in pred_test),\n",
    "    list(np.quantile(p[1], q = 0.75) for p in pred_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162437d-52a1-4f8a-840e-a639cfc74c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['rank_pred'] = myutils.rank_list(df_test['nscore_pred'], mode = 'center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea2035-d8fa-4f1e-9a3b-cb0f225b0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf503a-3e00-4e8a-b1fd-e60bba13ef15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
